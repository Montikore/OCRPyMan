{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Recognition with CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctc_utils as utils\n",
    "from importlib import reload \n",
    "import warnings\n",
    "\n",
    "# Ensure we have always the latest state and \n",
    "# not the last import in memory\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (32, 128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Part\n",
    "\n",
    "\n",
    "The idea here is to cut our image into several features (could be interpreted as smaller areas of the picture).\n",
    "\n",
    "<img src=\"imgs/cnn_result.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we ill use a succession of \n",
    "- **Convolution** to extract\n",
    "- **Batch Normalization** to prevent our model from overfitting (equivalent of Dropout) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch normalization\n",
    "\n",
    "TLDR: Solve **internal covariate shift** and simplify \n",
    "\n",
    "**Normalization** is a procedure to change the value of the numeric variable in the dataset to a typical scale, without misshaping contrasts in the range of value.\n",
    "\n",
    "**Batch normalization** is a technique for training very deep neural networks that normalizes the contributions to a layer for every mini-batch.\n",
    "\n",
    "In neural networks, the output of the first layer feeds into the second layer, the output of the second layer feeds into the third, and so on. When the parameters of a layer change, so does **the distribution of inputs to subsequent layers**.\n",
    "\n",
    "> We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training.\n",
    "\n",
    "These shifts in input distributions can be problematic for neural networks, especially deep neural networks that could have a large number of layers.\n",
    "\n",
    "Batch normalization is a method intended to mitigate internal covariate shift for neural networks.\n",
    "\n",
    "<img src=\"imgs/batch_normalization.jpeg\" />\n",
    "\n",
    "This has the impact of settling the learning process and drastically decreasing the number of training epochs required to train deep neural networks.\n",
    "\n",
    "Sources: \n",
    "- https://machinelearning.wtf/terms/internal-covariate-shift/\n",
    "- https://towardsdatascience.com/batch-normalisation-in-deep-neural-network-ce65dd9e8dbf#:~:text=Batch%20normalization%20solves%20a%20major,you%20can%20often%20remove%20dropout.\n",
    "- https://towardsdatascience.com/understanding-dataset-shift-f2a5a262a766"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv\n",
    "\n",
    "<img src=\"imgs/conv_padding.gif\" />\n",
    "\n",
    "- Padding '**valid**' is the first figure. The filter window stays inside the image. When padding == \"VALID\", there can be a loss of information. Generally, elements on the right and the bottom of the image tend to be ignored. How many elements are ignored depends on the size of the kernel and the stride.\n",
    "\n",
    "- Padding '**same**' is the third figure. The output is the same size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling/Flattening\n",
    "\n",
    "**Pooling** is the process of merging. So it’s basically for the purpose of **reducing the size of the data**.\n",
    "\n",
    "<img src=\"imgs/pooling_flattening.png\" />\n",
    "\n",
    "**Flattening** is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector.\n",
    "\n",
    "Sources: \n",
    "\n",
    "- https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-convolutional-neural-network-3607be47480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, LeakyReLU, Dropout\n",
    "\n",
    "\n",
    "hidden_layer_count = 256\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(5,5),\n",
    "        padding='SAME',\n",
    "        input_shape = (img_size[0], img_size[1], 1)\n",
    "    )\n",
    ")\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), padding='SAME'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='SAME'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(1,2), strides=(1,2)))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='SAME'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(1,2), strides=(1,2)))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='SAME'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(1,2), strides=(1,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have cut our entry image into X smaller parts. Those parts can be read as a time serie.\n",
    "\n",
    "Indeed, there is a strong connection between one part and the one coming after by construction (a word is a succession of letters).\n",
    "\n",
    "We can therefore make use of a **RNN** now in our model.\n",
    "\n",
    "NB: a part does not correspond exactly to a letter, it is more the idea behind it. It could be interesting to see the importance of the size of each part (number of cuts).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU, Bidirectional, Dense, Lambda\n",
    "\n",
    "# Remove axis 2\n",
    "model.add(Lambda(lambda x :tf.squeeze(x, axis=2)))\n",
    "# Bidirectionnal RNN\n",
    "model.add(Bidirectional(GRU(hidden_layer_count, return_sequences=True)))\n",
    "model.add(Dense(100))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO LSTM could be a good idea ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectionist Temporal Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels, charList):\n",
    "    # Hash Table\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(\n",
    "            charList,\n",
    "            np.arange(len(charList)),\n",
    "            value_dtype=tf.int32\n",
    "        ),\n",
    "        -1,\n",
    "        name='char2id'\n",
    "    )\n",
    "    return table.lookup(\n",
    "    tf.compat.v1.string_split(labels, delimiter=''))\n",
    "\n",
    "a = encode_labels(y_train[:5], charList)\n",
    "tf.sparse.to_dense(a)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((np.expand_dims(X_train,-1), y_train))\n",
    "dataset = dataset.shuffle(1000).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.reduce_mean(\n",
    "            tf.nn.ctc_loss(\n",
    "                labels = labels,\n",
    "                logits = logits,\n",
    "                logit_length = [logits.shape[1]]*logits.shape[0],\n",
    "                label_length = None,\n",
    "                logits_time_major = False,\n",
    "                blank_index=-1\n",
    "            )\n",
    "        )\n",
    "\n",
    "def train_op(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Prédiction de notre modèle\n",
    "        y_pred = model(inputs, training=True)\n",
    "        # Calcule de l'erreur de notre modèle\n",
    "        loss_value = tf.reduce_mean(loss(targets, y_pred))\n",
    "       \n",
    "    # Calculer le gradient de la fonction de perte\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    # Descente de gradient\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # Retourner la valeur de la fonction de perte\n",
    "    return loss_value.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "659032454581496215ea079f20061774e38407e775639f7a4814d7b89284a104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
